{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yb_0BHBB0gDF",
        "outputId": "cd4b68fc-26b4-4952-81e7-2b8ca2fce115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'xtts'...\n",
            "remote: Enumerating objects: 530, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 530 (delta 2), reused 0 (delta 0), pack-reused 524\u001b[K\n",
            "Receiving objects: 100% (530/530), 739.66 KiB | 21.75 MiB/s, done.\n",
            "Resolving deltas: 100% (293/293), done.\n",
            "/content/xtts\n",
            "Collecting TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1 (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/coqui-ai/TTS (to revision v0.21.1) to /tmp/pip-install-yysdhifi/tts_0a04cd27808e4a4a8d0c654b9c9e2145\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/coqui-ai/TTS /tmp/pip-install-yysdhifi/tts_0a04cd27808e4a4a8d0c654b9c9e2145\n",
            "  Running command git checkout -q 00a870c26abdc06429ffef3e2814b1a1d5b40fff\n",
            "  Resolved https://github.com/coqui-ai/TTS to commit 00a870c26abdc06429ffef3e2814b1a1d5b40fff\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydantic==1.10.13 (from -r requirements.txt (line 3))\n",
            "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart==0.0.6 (from -r requirements.txt (line 4))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.10.0)\n",
            "Collecting cutlet (from -r requirements.txt (line 6))\n",
            "  Downloading cutlet-0.4.0.tar.gz (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mecab-python3==1.0.6 (from -r requirements.txt (line 7))\n",
            "  Downloading mecab_python3-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidic-lite==1.0.8 (from -r requirements.txt (line 8))\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidic==1.1.0 (from -r requirements.txt (line 9))\n",
            "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langid (from -r requirements.txt (line 10))\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deepspeed (from -r requirements.txt (line 11))\n",
            "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydub (from -r requirements.txt (line 12))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from unidic==1.1.0->-r requirements.txt (line 9)) (2.31.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from unidic==1.1.0->-r requirements.txt (line 9)) (4.66.2)\n",
            "Collecting wasabi<1.0.0,>=0.6.0 (from unidic==1.1.0->-r requirements.txt (line 9))\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting plac<2.0.0,>=1.1.3 (from unidic==1.1.0->-r requirements.txt (line 9))\n",
            "  Downloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.2.1+cu121)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.10.1)\n",
            "Collecting scikit-learn>=1.3.0 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (7.0.0)\n",
            "Collecting anyascii>=0.3.0 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.9.3)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.2.5)\n",
            "Collecting pysbd>=0.3.4 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn>=0.5.1 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas<2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.7.1)\n",
            "Collecting trainer>=0.0.32 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coqpit>=0.0.16 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.42.1)\n",
            "Collecting pypinyin (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading pypinyin-0.51.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hangul-romanize (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Collecting gruut[de,es,fr]==2.2.3 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.8.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Collecting bangla (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting bnnumerizer (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading bnunicodenormalizer-0.1.6.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops>=0.6.0 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (4.38.2)\n",
            "Collecting encodec>=0.1.1 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy[ja]>=3 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.7.4)\n",
            "Collecting numpy==1.22.0 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.58.1)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.14.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut-ipa<1.0,>=0.12.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaconv (from cutlet->-r requirements.txt (line 6))\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fugashi (from cutlet->-r requirements.txt (line 6))\n",
            "  Downloading fugashi-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mojimoji (from cutlet->-r requirements.txt (line 6))\n",
            "  Downloading mojimoji-0.0.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hjson (from deepspeed->-r requirements.txt (line 11))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed->-r requirements.txt (line 11))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 11)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 11)) (9.0.0)\n",
            "Collecting pynvml (from deepspeed->-r requirements.txt (line 11))\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa>=0.10.0 (from TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading librosa-0.10.0.post1-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.4->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic==1.1.0->-r requirements.txt (line 9)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic==1.1.0->-r requirements.txt (line 9)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic==1.1.0->-r requirements.txt (line 9)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic==1.1.0->-r requirements.txt (line 9)) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (8.2.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (6.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.3.0)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading SudachiDict_core-20240109-py3-none-any.whl (71.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.15.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.4.2)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2))\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask>=2.0.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut[de,es,fr]==2.2.3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.10.0->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy[ja]>=3->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS@ git+https://github.com/coqui-ai/TTS@v0.21.1->-r requirements.txt (line 2)) (3.2.2)\n",
            "Building wheels for collected packages: unidic-lite, unidic, TTS, cutlet, langid, deepspeed, encodec, umap-learn, bnnumerizer, bnunicodenormalizer, jaconv, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr, gruut\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=bf733ef5d0069d212f155e62fe93cadef0a78efd54df6c146319991d34b5047c\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7406 sha256=2c08c60ae0847d8236c7b8167d1a1396063d5f5d88212908fbf360a3ce48e596\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/72/72/1f3d654c345ea69d5d51b531c90daf7ba14cc555eaf2c64ab0\n",
            "  Building wheel for TTS (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TTS: filename=TTS-0.21.1-cp310-cp310-linux_x86_64.whl size=898698 sha256=5caf173dc90376185196d0348bf627fd2b448d8aa2a1bf66f5084578fbd383eb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h9ewcpk7/wheels/82/c3/55/5867000d5ab1d8e6d1e84e6026b4e08b662facdfd0ce130820\n",
            "  Building wheel for cutlet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cutlet: filename=cutlet-0.4.0-py3-none-any.whl size=11677 sha256=0caf36ea2688b3b9482d3e5899bca6b306c2beefabb7640d8daa48118f8c40b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/29/e4/7b41fa990a65ee871955d9700653236223ae1abdfdf9244e53\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=fdb3d1c89c4c3c181a84c58b1870712a7461a2521b63b7ecf0f4c069968edaa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400401 sha256=efa61377043ce475511a368ee343e7e83fb8f752c537570f4cabd621e828dde3\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45762 sha256=4a93ed74f6433f783fa394a951fd1adb8699e30a8f97b04b94ba7151d4a31edf\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=e816897d7f964bf3efff9797ef0cecd86161bd6f9551c2eaa2b1bdc00e15c040\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=72e3b2b162e8f4e75b84b7af0b4c6608cf58c94c11f08b9490114a6168625451\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.6-py3-none-any.whl size=22780 sha256=65cc14b1f4d42f5d960ede1ae183ee5ffdca2f9471b33f758412751463a3c154\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/d7/e9/16732a619cbf5a63fdc9f6e2f9eb5fcf73fa023735237330e9\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=a0d58a2ba17673c9ae778014fb8e30633e04110f72f834f964e3bd3f11f0dc1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a8d6bbc76faf13b33a07c345d1f30896e9934f7f294b971f3096907126830907\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104870 sha256=6ab96690fde15cb182a88a6dc05ac426ecb95979ed65d1b57d9c042219d39faf\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498183 sha256=fe8dcd26577d6cc3a51b74bf3420daefd99ba1461d4f4ab9a9c3d117573472af\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297178 sha256=4801c39f7a5ac26e99b4f985b7e6d02021c6bf6ed4c59b0484239ddeac4351b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173797 sha256=13fdc693acbd788c4c6c210f7872c3959c5cda3d3994890bfaac0e73526c55ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/0a/90/788d92c07744b329b9283e37b29b064f5db6b1bb0442a1a19b\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968767 sha256=6fd721cc8a23d044da62fc27651f3df125477685e8157401213dcf405e1cf49d\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75791 sha256=7d0dd54885596b62f83b1f9b9b19ea2fa1b3df1553f628e7fbe3944204831dbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "Successfully built unidic-lite unidic TTS cutlet langid deepspeed encodec umap-learn bnnumerizer bnunicodenormalizer jaconv docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr gruut\n",
            "Installing collected packages: wasabi, unidic-lite, sudachipy, python-crfsuite, pydub, plac, ninja, mecab-python3, jamo, jaconv, hjson, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-multipart, pysbd, pypinyin, pynvml, pydantic, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, num2words, networkx, mojimoji, jsonlines, gruut-ipa, fugashi, einops, coqpit, anyascii, unidic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, langid, g2pkk, dateparser, cutlet, scikit-learn, nvidia-cusolver-cu12, gruut, pynndescent, librosa, umap-learn, trainer, deepspeed, encodec, TTS\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.4\n",
            "    Uninstalling pydantic-2.6.4:\n",
            "      Successfully uninstalled pydantic-2.6.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.1\n",
            "    Uninstalling librosa-0.10.1:\n",
            "      Successfully uninstalled librosa-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.21.1 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.6 coqpit-0.0.17 cutlet-0.4.0 dateparser-1.1.8 deepspeed-0.14.0 docopt-0.6.2 einops-0.7.0 encodec-0.1.1 fugashi-1.3.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 gruut_lang_es-2.0.0 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 hjson-3.1.0 jaconv-0.3.4 jamo-0.4.1 jsonlines-1.2.0 langid-1.1.6 librosa-0.10.0 mecab-python3-1.0.6 mojimoji-0.0.13 networkx-2.8.8 ninja-1.11.1.1 num2words-0.5.13 numpy-1.22.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 plac-1.4.3 pydantic-1.10.13 pydub-0.25.1 pynndescent-0.5.12 pynvml-11.5.0 pypinyin-0.51.0 pysbd-0.3.4 python-crfsuite-0.9.10 python-multipart-0.0.6 scikit-learn-1.4.1.post1 sudachidict-core-20240109 sudachipy-0.6.8 trainer-0.0.36 umap-learn-0.5.5 unidecode-1.3.8 unidic-1.1.0 unidic-lite-1.0.8 wasabi-0.10.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0abaf9568601496ebe8d9b9da84f4cec",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.24.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.14.0 (from gradio)\n",
            "  Downloading gradio_client-0.14.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.0)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.14.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=4d74e1af2193cdd4f69dca17e18d1c0fdfcc2c0451ce140a06e0b7ec714f89eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, pydantic, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: python-multipart\n",
            "    Found existing installation: python-multipart 0.0.6\n",
            "    Uninstalling python-multipart-0.0.6:\n",
            "      Successfully uninstalled python-multipart-0.0.6\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.24.0 gradio-client-0.14.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.0 pydantic-2.6.4 python-multipart-0.0.9 ruff-0.3.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/spaces/coqui/xtts.git\n",
        "%cd xtts\n",
        "!pip install -r requirements.txt\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5lJ3VjY4xim",
        "outputId": "98a76cd3-d54c-4911-b03b-59a1b5ce1cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export newer ffmpeg binary for denoise filter\n",
            "Make ffmpeg binary executable\n",
            "Downloading if not downloaded Coqui XTTS V2\n",
            " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
            "XTTS downloaded\n",
            "[2024-03-31 12:43:48,354] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
            "[2024-03-31 12:43:48,357] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2024-03-31 12:43:48,361] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
            "[2024-03-31 12:43:48,365] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "[2024-03-31 12:43:48,639] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import io, os, stat\n",
        "import subprocess\n",
        "import random\n",
        "from zipfile import ZipFile\n",
        "import uuid\n",
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "\n",
        "#download for mecab\n",
        "os.system('python -m unidic download')\n",
        "\n",
        "# By using XTTS you agree to CPML license https://coqui.ai/cpml\n",
        "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
        "\n",
        "# langid is used to detect language for longer text\n",
        "# Most users expect text to be their own language, there is checkbox to disable it\n",
        "import langid\n",
        "import base64\n",
        "import csv\n",
        "from io import StringIO\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "import gradio as gr\n",
        "from scipy.io.wavfile import write\n",
        "from pydub import AudioSegment\n",
        "\n",
        "from TTS.api import TTS\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "from TTS.utils.generic_utils import get_user_data_dir\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# will use api to restart space on a unrecoverable error\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "repo_id = \"coqui/xtts\"\n",
        "\n",
        "# Use never ffmpeg binary for Ubuntu20 to use denoising for microphone input\n",
        "print(\"Export newer ffmpeg binary for denoise filter\")\n",
        "ZipFile(\"ffmpeg.zip\").extractall()\n",
        "print(\"Make ffmpeg binary executable\")\n",
        "st = os.stat(\"ffmpeg\")\n",
        "os.chmod(\"ffmpeg\", st.st_mode | stat.S_IEXEC)\n",
        "\n",
        "# This will trigger downloading model\n",
        "print(\"Downloading if not downloaded Coqui XTTS V2\")\n",
        "from TTS.utils.manage import ModelManager\n",
        "\n",
        "model_name = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "ModelManager().download_model(model_name)\n",
        "model_path = os.path.join(get_user_data_dir(\"tts\"), model_name.replace(\"/\", \"--\"))\n",
        "print(\"XTTS downloaded\")\n",
        "\n",
        "config = XttsConfig()\n",
        "config.load_json(os.path.join(model_path, \"config.json\"))\n",
        "\n",
        "model = Xtts.init_from_config(config)\n",
        "model.load_checkpoint(\n",
        "    config,\n",
        "    checkpoint_path=os.path.join(model_path, \"model.pth\"),\n",
        "    vocab_path=os.path.join(model_path, \"vocab.json\"),\n",
        "    eval=True,\n",
        "    use_deepspeed=True,\n",
        ")\n",
        "model.cuda()\n",
        "\n",
        "# This is for debugging purposes only\n",
        "DEVICE_ASSERT_DETECTED = 0\n",
        "DEVICE_ASSERT_PROMPT = None\n",
        "DEVICE_ASSERT_LANG = None\n",
        "\n",
        "supported_languages = config.languages\n",
        "\n",
        "def predict(\n",
        "    prompt,\n",
        "    language,\n",
        "    audio_file_pth,\n",
        "    save_path\n",
        "):\n",
        "    voice_cleanup = False\n",
        "    mic_file_path = None\n",
        "    use_mic = False\n",
        "    agree = True\n",
        "    no_lang_auto_detect = False\n",
        "    if agree == True:\n",
        "        if language not in supported_languages:\n",
        "            gr.Warning(\n",
        "                f\"Language you put {language} in is not in is not in our Supported Languages, please choose from dropdown\"\n",
        "            )\n",
        "\n",
        "            return (\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "            )\n",
        "\n",
        "        language_predicted = langid.classify(prompt)[\n",
        "            0\n",
        "        ].strip()  # strip need as there is space at end!\n",
        "\n",
        "        # tts expects chinese as zh-cn\n",
        "        if language_predicted == \"zh\":\n",
        "            # we use zh-cn\n",
        "            language_predicted = \"zh-cn\"\n",
        "\n",
        "        print(f\"Detected language:{language_predicted}, Chosen language:{language}\")\n",
        "\n",
        "        # After text character length 15 trigger language detection\n",
        "        if len(prompt) > 15:\n",
        "            # allow any language for short text as some may be common\n",
        "            # If user unchecks language autodetection it will not trigger\n",
        "            # You may remove this completely for own use\n",
        "            if language_predicted != language and not no_lang_auto_detect:\n",
        "                # Please duplicate and remove this check if you really want this\n",
        "                # Or auto-detector fails to identify language (which it can on pretty short text or mixed text)\n",
        "                gr.Warning(\n",
        "                    f\"It looks like your text isn’t the language you chose , if you’re sure the text is the same language you chose, please check disable language auto-detection checkbox\"\n",
        "                )\n",
        "\n",
        "                return (\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                )\n",
        "\n",
        "        if use_mic == True:\n",
        "            if mic_file_path is not None:\n",
        "                speaker_wav = mic_file_path\n",
        "            else:\n",
        "                gr.Warning(\n",
        "                    \"Please record your voice with Microphone, or uncheck Use Microphone to use reference audios\"\n",
        "                )\n",
        "                return (\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            speaker_wav = audio_file_pth\n",
        "\n",
        "        # Filtering for microphone input, as it has BG noise, maybe silence in beginning and end\n",
        "        # This is fast filtering not perfect\n",
        "\n",
        "        # Apply all on demand\n",
        "        lowpassfilter = denoise = trim = loudness = True\n",
        "\n",
        "        if lowpassfilter:\n",
        "            lowpass_highpass = \"lowpass=8000,highpass=75,\"\n",
        "        else:\n",
        "            lowpass_highpass = \"\"\n",
        "\n",
        "        if trim:\n",
        "            # better to remove silence in beginning and end for microphone\n",
        "            trim_silence = \"areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,\"\n",
        "        else:\n",
        "            trim_silence = \"\"\n",
        "\n",
        "        if voice_cleanup:\n",
        "            try:\n",
        "                out_filename = (\n",
        "                    speaker_wav + str(uuid.uuid4()) + \".wav\"\n",
        "                )  # ffmpeg to know output format\n",
        "\n",
        "                # we will use newer ffmpeg as that has afftn denoise filter\n",
        "                shell_command = f\"./ffmpeg -y -i {speaker_wav} -af {lowpass_highpass}{trim_silence} {out_filename}\".split(\n",
        "                    \" \"\n",
        "                )\n",
        "\n",
        "                command_result = subprocess.run(\n",
        "                    [item for item in shell_command],\n",
        "                    capture_output=False,\n",
        "                    text=True,\n",
        "                    check=True,\n",
        "                )\n",
        "                speaker_wav = out_filename\n",
        "                print(\"Filtered microphone input\")\n",
        "            except subprocess.CalledProcessError:\n",
        "                # There was an error - command exited with non-zero code\n",
        "                print(\"Error: failed filtering, use original microphone input\")\n",
        "        else:\n",
        "            speaker_wav = speaker_wav\n",
        "\n",
        "        if len(prompt) < 2:\n",
        "            gr.Warning(\"Please give a longer prompt text\")\n",
        "            return (\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "            )\n",
        "        if len(prompt) > 500:\n",
        "            gr.Warning(\n",
        "                \"Text length limited to 500 characters for this demo, please try shorter text. You can clone this space and edit code for your own usage\"\n",
        "            )\n",
        "            return (\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "            )\n",
        "        global DEVICE_ASSERT_DETECTED\n",
        "        if DEVICE_ASSERT_DETECTED:\n",
        "            global DEVICE_ASSERT_PROMPT\n",
        "            global DEVICE_ASSERT_LANG\n",
        "            # It will likely never come here as we restart space on first unrecoverable error now\n",
        "            print(\n",
        "                f\"Unrecoverable exception caused by language:{DEVICE_ASSERT_LANG} prompt:{DEVICE_ASSERT_PROMPT}\"\n",
        "            )\n",
        "\n",
        "            # HF Space specific.. This error is unrecoverable need to restart space\n",
        "            space = api.get_space_runtime(repo_id=repo_id)\n",
        "            if space.stage!=\"BUILDING\":\n",
        "                api.restart_space(repo_id=repo_id)\n",
        "            else:\n",
        "                print(\"TRIED TO RESTART but space is building\")\n",
        "\n",
        "        try:\n",
        "            metrics_text = \"\"\n",
        "            t_latent = time.time()\n",
        "\n",
        "            # note diffusion_conditioning not used on hifigan (default mode), it will be empty but need to pass it to model.inference\n",
        "            try:\n",
        "                (\n",
        "                    gpt_cond_latent,\n",
        "                    speaker_embedding,\n",
        "                ) = model.get_conditioning_latents(audio_path=speaker_wav, gpt_cond_len=30, gpt_cond_chunk_len=4, max_ref_length=60)\n",
        "            except Exception as e:\n",
        "                print(\"Speaker encoding error\", str(e))\n",
        "                gr.Warning(\n",
        "                    \"It appears something wrong with reference, did you unmute your microphone?\"\n",
        "                )\n",
        "                return (\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                )\n",
        "\n",
        "            latent_calculation_time = time.time() - t_latent\n",
        "            # metrics_text=f\"Embedding calculation time: {latent_calculation_time:.2f} seconds\\n\"\n",
        "\n",
        "            # temporary comma fix\n",
        "            prompt= re.sub(\"([^\\x00-\\x7F]|\\w)(\\.|\\。|\\?)\",r\"\\1 \\2\\2\",prompt)\n",
        "\n",
        "            wav_chunks = []\n",
        "            ## Direct mode\n",
        "\n",
        "            print(\"I: Generating new audio...\")\n",
        "            t0 = time.time()\n",
        "            out = model.inference(\n",
        "                prompt,\n",
        "                language,\n",
        "                gpt_cond_latent,\n",
        "                speaker_embedding,\n",
        "                repetition_penalty=5.0,\n",
        "                temperature=0.75,\n",
        "            )\n",
        "            inference_time = time.time() - t0\n",
        "            print(f\"I: Time to generate audio: {round(inference_time*1000)} milliseconds\")\n",
        "            metrics_text+=f\"Time to generate audio: {round(inference_time*1000)} milliseconds\\n\"\n",
        "            real_time_factor= (time.time() - t0) / out['wav'].shape[-1] * 24000\n",
        "            print(f\"Real-time factor (RTF): {real_time_factor}\")\n",
        "            metrics_text+=f\"Real-time factor (RTF): {real_time_factor:.2f}\\n\"\n",
        "            torchaudio.save(f\"output/{save_path}.wav\", torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"I: Generating new audio in streaming mode...\")\n",
        "            t0 = time.time()\n",
        "            chunks = model.inference_stream(\n",
        "                prompt,\n",
        "                language,\n",
        "                gpt_cond_latent,\n",
        "                speaker_embedding,\n",
        "                repetition_penalty=7.0,\n",
        "                temperature=0.85,\n",
        "            )\n",
        "            first_chunk = True\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                if first_chunk:\n",
        "                    first_chunk_time = time.time() - t0\n",
        "                    metrics_text += f\"Latency to first audio chunk: {round(first_chunk_time*1000)} milliseconds\\n\"\n",
        "                    first_chunk = False\n",
        "                wav_chunks.append(chunk)\n",
        "                print(f\"Received chunk {i} of audio length {chunk.shape[-1]}\")\n",
        "            inference_time = time.time() - t0\n",
        "            print(\n",
        "                f\"I: Time to generate audio: {round(inference_time*1000)} milliseconds\"\n",
        "            )\n",
        "            #metrics_text += (\n",
        "            #    f\"Time to generate audio: {round(inference_time*1000)} milliseconds\\n\"\n",
        "            #)\n",
        "            wav = torch.cat(wav_chunks, dim=0)\n",
        "            print(wav.shape)\n",
        "            real_time_factor = (time.time() - t0) / wav.shape[0] * 24000\n",
        "            print(f\"Real-time factor (RTF): {real_time_factor}\")\n",
        "            metrics_text += f\"Real-time factor (RTF): {real_time_factor:.2f}\\n\"\n",
        "            torchaudio.save(\"output.wav\", wav.squeeze().unsqueeze(0).cpu(), 24000)\n",
        "            \"\"\"\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"device-side assert\" in str(e):\n",
        "                # cannot do anything on cuda device side error, need tor estart\n",
        "                print(\n",
        "                    f\"Exit due to: Unrecoverable exception caused by language:{language} prompt:{prompt}\",\n",
        "                    flush=True,\n",
        "                )\n",
        "                gr.Warning(\"Unhandled Exception encounter, please retry in a minute\")\n",
        "                print(\"Cuda device-assert Runtime encountered need restart\")\n",
        "                if not DEVICE_ASSERT_DETECTED:\n",
        "                    DEVICE_ASSERT_DETECTED = 1\n",
        "                    DEVICE_ASSERT_PROMPT = prompt\n",
        "                    DEVICE_ASSERT_LANG = language\n",
        "\n",
        "                # just before restarting save what caused the issue so we can handle it in future\n",
        "                # Uploading Error data only happens for unrecovarable error\n",
        "                error_time = datetime.datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
        "                error_data = [\n",
        "                    error_time,\n",
        "                    prompt,\n",
        "                    language,\n",
        "                    audio_file_pth,\n",
        "                    mic_file_path,\n",
        "                    use_mic,\n",
        "                    voice_cleanup,\n",
        "                    no_lang_auto_detect,\n",
        "                    agree,\n",
        "                ]\n",
        "                error_data = [str(e) if type(e) != str else e for e in error_data]\n",
        "                print(error_data)\n",
        "                print(speaker_wav)\n",
        "                write_io = StringIO()\n",
        "                csv.writer(write_io).writerows([error_data])\n",
        "                csv_upload = write_io.getvalue().encode()\n",
        "\n",
        "                filename = error_time + \"_\" + str(uuid.uuid4()) + \".csv\"\n",
        "                print(\"Writing error csv\")\n",
        "                error_api = HfApi()\n",
        "                error_api.upload_file(\n",
        "                    path_or_fileobj=csv_upload,\n",
        "                    path_in_repo=filename,\n",
        "                    repo_id=\"coqui/xtts-flagged-dataset\",\n",
        "                    repo_type=\"dataset\",\n",
        "                )\n",
        "\n",
        "                # speaker_wav\n",
        "                print(\"Writing error reference audio\")\n",
        "                speaker_filename = (\n",
        "                    error_time + \"_reference_\" + str(uuid.uuid4()) + \".wav\"\n",
        "                )\n",
        "                error_api = HfApi()\n",
        "                error_api.upload_file(\n",
        "                    path_or_fileobj=speaker_wav,\n",
        "                    path_in_repo=speaker_filename,\n",
        "                    repo_id=\"coqui/xtts-flagged-dataset\",\n",
        "                    repo_type=\"dataset\",\n",
        "                )\n",
        "\n",
        "                # HF Space specific.. This error is unrecoverable need to restart space\n",
        "                space = api.get_space_runtime(repo_id=repo_id)\n",
        "                if space.stage!=\"BUILDING\":\n",
        "                    api.restart_space(repo_id=repo_id)\n",
        "                else:\n",
        "                    print(\"TRIED TO RESTART but space is building\")\n",
        "\n",
        "            else:\n",
        "                if \"Failed to decode\" in str(e):\n",
        "                    print(\"Speaker encoding error\", str(e))\n",
        "                    gr.Warning(\n",
        "                        \"It appears something wrong with reference, did you unmute your microphone?\"\n",
        "                    )\n",
        "                else:\n",
        "                    print(\"RuntimeError: non device-side assert error:\", str(e))\n",
        "                    gr.Warning(\"Something unexpected happened please retry again.\")\n",
        "                return (\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                )\n",
        "        return (\n",
        "            f\"output/{save_path}.wav\"\n",
        "        )\n",
        "    else:\n",
        "        gr.Warning(\"Please accept the Terms & Condition!\")\n",
        "        return (\n",
        "            None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class subtitle:\n",
        "    def __init__(self,index:int, start_time, end_time, text:str):\n",
        "        self.index = int(index)\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time\n",
        "        self.text = text.strip()\n",
        "    def normalize(self,ntype:str,fps=30):\n",
        "         if ntype==\"prcsv\":\n",
        "              h,m,s,fs=(self.start_time.replace(';',':')).split(\":\")#seconds\n",
        "              self.start_time=int(h)*3600+int(m)*60+int(s)+round(int(fs)/fps,2)\n",
        "              h,m,s,fs=(self.end_time.replace(';',':')).split(\":\")\n",
        "              self.end_time=int(h)*3600+int(m)*60+int(s)+round(int(fs)/fps,2)\n",
        "         elif ntype==\"srt\":\n",
        "             h,m,s=self.start_time.split(\":\")\n",
        "             s=s.replace(\",\",\".\")\n",
        "             self.start_time=int(h)*3600+int(m)*60+round(float(s),2)\n",
        "             h,m,s=self.end_time.split(\":\")\n",
        "             s=s.replace(\",\",\".\")\n",
        "             self.end_time=int(h)*3600+int(m)*60+round(float(s),2)\n",
        "         else:\n",
        "             raise ValueError\n",
        "    def add_offset(self,offset=0):\n",
        "        self.start_time+=offset\n",
        "        if self.start_time<0:\n",
        "            self.start_time=0\n",
        "        self.end_time+=offset\n",
        "        if self.end_time<0:\n",
        "            self.end_time=0\n",
        "    def __str__(self) -> str:\n",
        "        return f'id:{self.index},start:{self.start_time},end:{self.end_time},text:{self.text}'\n",
        "\n",
        "def read_srt(filename):\n",
        "    offset=0\n",
        "    with open(filename,\"r\",encoding=\"utf-8\") as f:\n",
        "        file=f.readlines()\n",
        "    subtitle_list=[]\n",
        "    indexlist=[]\n",
        "    filelength=len(file)\n",
        "    for i in range(0,filelength):\n",
        "        if \" --> \" in file[i]:\n",
        "            is_st=True\n",
        "            for char in file[i-1].strip().replace(\"\\ufeff\",\"\"):\n",
        "                if char not in ['0','1','2','3','4','5','6','7','8','9']:\n",
        "                    is_st=False\n",
        "                    break\n",
        "            if is_st:\n",
        "                indexlist.append(i) #get line id\n",
        "    listlength=len(indexlist)\n",
        "    for i in range(0,listlength-1):\n",
        "        st,et=file[indexlist[i]].split(\" --> \")\n",
        "        id=int(file[indexlist[i]-1].strip().replace(\"\\ufeff\",\"\"))\n",
        "        text=\"\"\n",
        "        for x in range(indexlist[i]+1,indexlist[i+1]-2):\n",
        "            text+=file[x]\n",
        "        st=subtitle(id,st,et,text)\n",
        "        st.normalize(ntype=\"srt\")\n",
        "        st.add_offset(offset=offset)\n",
        "        subtitle_list.append(st)\n",
        "    st,et=file[indexlist[-1]].split(\" --> \")\n",
        "    id=file[indexlist[-1]-1]\n",
        "    text=\"\"\n",
        "    for x in range(indexlist[-1]+1,filelength):\n",
        "        text+=file[x]\n",
        "    st=subtitle(id,st,et,text)\n",
        "    st.normalize(ntype=\"srt\")\n",
        "    st.add_offset(offset=offset)\n",
        "    subtitle_list.append(st)\n",
        "    return subtitle_list\n",
        "\n",
        "\n",
        "def convert_from_srt(filename, language):\n",
        "  subtitle_list = read_srt(filename)\n",
        "  for i in subtitle_list:\n",
        "    os.makedirs(\"output\", exist_ok=True)\n",
        "    print(f\"正在合成第{i.index}条语音\")\n",
        "    print(f\"语音内容：{i.text.splitlines()[0]}\")\n",
        "    predict(i.text.splitlines()[0], language, f\"{i.text.splitlines()[1]}.wav\", i.text.splitlines()[1] + \" \" + str(i.index))\n"
      ],
      "metadata": {
        "id": "HKBg_kQFdCYx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RVf4qshG9Rax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbfb2e2-6ada-4d36-9bbe-425454039eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在合成第1条语音\n",
            "语音内容：I love you, Alibaba.\n",
            "Detected language:en, Chosen language:en\n",
            "I: Generating new audio...\n",
            "I: Time to generate audio: 424 milliseconds\n",
            "Real-time factor (RTF): 0.2860788175527998\n",
            "正在合成第2条语音\n",
            "语音内容：I love artificial intelligence. What about you?\n",
            "Detected language:en, Chosen language:en\n",
            "I: Generating new audio...\n",
            "I: Time to generate audio: 893 milliseconds\n",
            "Real-time factor (RTF): 0.24715461562165117\n",
            "正在合成第3条语音\n",
            "语音内容：I love you, Nvidia.\n",
            "Detected language:en, Chosen language:en\n",
            "I: Generating new audio...\n",
            "I: Time to generate audio: 459 milliseconds\n",
            "Real-time factor (RTF): 0.38120915404463235\n"
          ]
        }
      ],
      "source": [
        "convert_from_srt(\"subtitle.srt\", \"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q3KCEwP3WF-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}